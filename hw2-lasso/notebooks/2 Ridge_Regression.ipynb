{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "# train_X = pd.read_csv('../p1Data/train_X.csv',header=None)\n",
    "# train_y = pd.read_csv('../p1Data/train_y.csv',header=None)\n",
    "# validation_X = pd.read_csv('../p1Data/validation_X.csv',header=None)\n",
    "# validation_y = pd.read_csv('../p1Data/validation_y.csv',header=None)\n",
    "train_X = np.genfromtxt('../p1Data/train_X.csv',delimiter=',')\n",
    "train_y = np.genfromtxt('../p1Data/train_y.csv',delimiter=',')\n",
    "validation_X = np.genfromtxt('../p1Data/validation_X.csv',delimiter=',')\n",
    "validation_y = np.genfromtxt('../p1Data/validation_y.csv',delimiter=',')\n",
    "test_X = np.genfromtxt('../p1Data/test_X.csv',delimiter=',')\n",
    "test_y = np.genfromtxt('../p1Data/test_y.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use both sklearn.optimize.minimized() function and the sklearn.linear_model.Ridge to run ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklear.optimize.minimize() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0.0173270678551\n",
      "0.0001 0.0534227742847\n",
      "0.001 0.384204784306\n",
      "0.01 1.6785955378\n",
      "0.1 4.99419973261\n",
      "1 7.78280178024\n",
      "10 49.2569463265\n",
      "100 154.88721917\n",
      "1000 182.896572549\n",
      "10000 186.111513059\n",
      "100000 186.437683461\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "X = np.loadtxt('../data/X_train.txt')\n",
    "y = np.loadtxt('../data/y_train.txt')\n",
    "X_val= np.loadtxt('../data/X_valid.txt')\n",
    "y_val= np.loadtxt('../data/y_valid.txt')\n",
    "\n",
    "(N,D) = X.shape\n",
    "\n",
    "w = np.random.rand(D,1)\n",
    "\n",
    "def ridge(Lambda):\n",
    "    def ridge_obj(theta):\n",
    "        return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N) + Lambda*(np.linalg.norm(theta))**2\n",
    "    return ridge_obj\n",
    "\n",
    "def compute_loss(Lambda, theta):\n",
    "    return ((np.linalg.norm(np.dot(X_val,theta) - y_val))**2)/(2*N)\n",
    "\n",
    "for i in range(-5,6):\n",
    "    Lambda = 10**i;\n",
    "    w_opt = minimize(ridge(Lambda), w)\n",
    "    print( Lambda, compute_loss(Lambda, w_opt.x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose $\\lambda = 1e-5$ since it achieves best valdiation square loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_opt = minimize(ridge(1e-5), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.94244415e+00,   9.83567726e+00,  -9.76393854e+00,\n",
       "        -9.84368059e+00,  -9.97545839e+00,  -1.00054636e+01,\n",
       "        -1.00492982e+01,  -1.00654249e+01,  -9.93792709e+00,\n",
       "        -9.90725056e+00,   7.86432619e-02,   2.64012171e-01,\n",
       "         1.73014901e-01,  -3.87271967e-02,   1.46982996e-02,\n",
       "         1.05442100e-01,  -3.10636870e-01,  -1.15626912e-02,\n",
       "        -2.56517590e-01,  -9.16715855e-02,   2.36415117e-02,\n",
       "        -5.12369093e-02,   1.63186951e-01,  -6.20184015e-02,\n",
       "        -2.74729567e-01,   2.08807502e-01,  -7.89390125e-02,\n",
       "         3.42368186e-01,   2.19937280e-01,  -1.28661134e-01,\n",
       "        -1.60172396e-01,  -1.53584379e-03,  -2.94891748e-02,\n",
       "        -1.24655657e-01,   6.63393209e-02,   3.13957765e-02,\n",
       "        -3.05021636e-01,   9.77729269e-02,   1.67797289e-01,\n",
       "        -2.91773672e-01,   1.53056090e-01,  -5.22732532e-02,\n",
       "         6.94282310e-02,   2.41839636e-03,   1.06592354e-01,\n",
       "         6.15879303e-02,   4.28718822e-02,   1.02156906e-01,\n",
       "         8.72525019e-03,   1.62550982e-02,   6.15201468e-02,\n",
       "        -1.13013923e-01,  -1.20882726e-01,  -1.45966092e-01,\n",
       "        -1.90685907e-02,   5.98411919e-02,  -1.26758097e-01,\n",
       "         2.22842478e-02,  -5.16096128e-03,   1.56190729e-01,\n",
       "        -7.56512875e-02,  -9.32535914e-02,  -1.60697825e-01,\n",
       "         6.02370476e-02,   4.70462944e-02,  -2.51040561e-01,\n",
       "        -2.21299013e-01,   8.22241106e-02,   1.17879284e-02,\n",
       "         1.16062244e-01,   3.56050316e-02,  -1.36535883e-01,\n",
       "        -1.82210193e-02,   2.25831124e-01,   8.76657098e-02])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_opt.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that although our optimized w does gives out close-to-zero estimation to those components whose true values being zero. But in the threshold of 0.001, we do not have sparsity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results with sklearn.linaer_model.Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2model = Ridge(alpha=0.1)\n",
    "l2model.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_y_predict = l2model.predict(validation_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2318688977252545"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set square loss\n",
    "diff = validation_y_predict-validation_y\n",
    "0.5/diff.shape[0]*np.dot(diff,diff.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built in ridge function gives out similar validation set loss when alpha/lambda_reg = 0.1."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
